### Convolutional Nerual Network for Sentence Classification

论文链接：https://www.aclweb.org/anthology/D14-1181



[1]: https://www.aclweb.org/anthology/D14-1181	"Convolutional Neural Network for Sentence Classification"



文中建立了一个一层卷积层和一层全连接层组成的网络，实现文本分类任务。

#### Introduction

在 Introduction 部分中，开始介绍了一些深度深度神经网络在 NLP 任务的应用，以及CNN网络在语义分析、问题检索等任务上都有不错的效果。

这一部分最后介绍了本文中建立的模型的一些特点，首先卷积层的处理是在词向量之上的，文中对预训练好的词向量以及在训练过程中动态调整的词向量（也就是随机初始化了词向量矩阵，然后也对这些词向量进行训练）都做了测试。相较于随机初始化的词向量，使用预先训练好的词向量的效果更好（这里使用的是 Mikolov 使用 Google News 进行训练得到的词向量），即使两种任务相差甚远，但是词向量是通用的，这些词向量都具有相似的特征。此外，文中所建立的模型中，有一个很重要的是使用了两个 “Channel” 的模型，也就是说，在卷积层的输入上，一个 channel 是使用的 Pre-train 的词向量，这些词向量在训练过程中是不会变化的，另一个 channel 则是动态词向量，可以根据训练进行 fine-tuning（大概是微调的意思）。

#### Model

这一部分详细介绍了 TextCNN 模型，其实与图像处理的卷积网络结构一样，Figure 1 展示了整个模型的运行情况，对于输入的一个文本，将词向量作为卷积处理的单位，当然定义的 filter 也就只能上下（也就是单词之间移动，不然横移动表示在词向量上进行移动，这样就破坏了词语的完整性，就没有意义了），在这张图上，也可以的看到之前提到的 channel 是什么意思，其实本来输入只有一层的，也就是 词语数量 * 词向量维度 这样的，相当于又加了一层，只不过一层可以调整，一层不能调整而已。

此外，模型中还使用了不同尺寸的卷积核，这有点像 GooLeNet 模型中提出的并行卷积处理结构， 也就是在不同的池逊上提取句子的特征，这种方式可以理解成统计机器学习模型的 N-Gram 模型，也就是相当于提取了词语之间的顺序关系，相对于只使用一种尺寸的卷积核，多种长度应该会得到更多信息，分类的效果应该会更好。

在经过卷积处理之后，接下来进行了一次 max-pool （最大池化），这里的池化的范围是整个特征图，可以想到，一个句子经过卷积处理与最大池化的依次处理之后，会成为一个 单值， 这里最大池化的解释是找到最重要的特征，这里最重要的特征也就是最大值了。。。（不过这样之后的参数倒是节省了不少）。

经过最大池化之后就是非常基操的展平后接全连接分类了，如果假设 K  中不同尺寸的 filter， 每种尺寸的 filter 有 n 个，那么展平后的维度就是 [batch_size, Kn ]，个这里假设每种尺寸都有 n 个了，如果数量之间不一样的话就是相加了。

最后对于全连接层，使用 dropuout 方法（实话说，我觉得这里的参数量也不算大啊。。）然后再训练结束之后对这一层的参数 ，也就是 W 参数矩阵进行了 L2 正则化处理。

#### Experiments

文章的第三部分是对使用的一些数据集进行介绍，在实验上，重点是文中进行测试所使用的不同模型，这里主要是对模型使用的词向量作为划分， 这一部分在 3.3中具体作了介绍，第一种是 CNN-rand，也就是使用完全随机初始化的词向量， CNN-static 使用的是 pre-train 的词向量，词向量在训练过程中是不可训练，也就是不改变的。第三种是 CNN-non-static，也就是使用的 pre-train 的词向量，但是这些词向量可以调整。最后一种是 CNN-multichannel ，也就是之前提到的，使用了两种 channel ，分别是不可调整和可以及逆行微调的。

对于结果分析部分，文中做了详细的数据对比。这四种方式中，可以看到即使是最简单的完全随机初始化的CNN结果，在结果上也是很不错的，不过还是可以看出，如果使用了 pre-train 的词向量，各个测试集上的数据结果都要提升了一些，相比之下，如果对这些 static 的词向量进行微调， 在其中一些数据集上的结果会再提高一些。

按照一般规律，放在最后的模型都会有 buff 加成，而且双 channel 确实感觉很厉害，但是数据集的结果却不是这样，从结果来看，这与使用进行微调的模型相比， 在大部分数据集上的表现反而不够好，当然这些差距都很小，只有两个数据集的效果更好一些，总之，不分伯仲，对于这个结果，文中也写道，因此，可以使用一个 extra dimension 代替双 channel。（这个地方我没有搞懂这个增加一个额外的维度是什么意思？）

在比较 static 和 non-static 的结果数据上，可以发现一些非常有趣的结果，文中列出了一些调整后的词向量以及它们最相近的单词，如果是static vector，good 与 bad 是非常相似的，因为它们的使用场景很一致，但是在具体数据集上进行微调之后，good 就与 nice 之类的词更为接近了。这反映了对于具体数据的学习过程，此外，对于随机初始化的词向量们，甚至可以看到标点符号相似的单词也是很不同的， 感叹号作为情感比较强烈的符号，就比较接近于一些表示情感的形容词，比如 beautiful terrible 等等。但是逗号就更接近于 but and 等等连词了。

在 Futher Observation 中，提到了 dropout 提高了模型的表现，可以提高大概 2%-4% ， 另外， 对于使用不同数据集上pre-train 的词向量也会影响模型表现，对于这个问题，文中也没有给出具体的解释。

#### Conclusion

结论部分，即使是一个非常简单的一层卷积处理的CNN就可以达到很好的结果，作者认为无监督预训练好的词向量将会成为 NLP 相关任务中的重要组成部分。

摘要对模型结构
