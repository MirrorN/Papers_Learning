### word2vec Parameter Learning Explained

论文链接：<http://wiki.hacksmeta.com/static/pdf/word2vec-Parameter-Learning-Explained-5.pdf>

这篇论文详细的介绍了 word2vec 参数的计算更新步骤，就像作者在摘要里所写的一样，Mikolov 的论文简化了公式的推导，所以文章显得比较晦涩。整篇论文可以分为三个部分吧，分别是：

- CBOW模型以及Skip-Gram模型在单输入以及多输入的情况下参数更新过程（这是没有使用优化计算技巧的情况）
- 两种优化计算的方式：Hierarchical Softmax 以及 Native Sampling 情况下参数的更新过程
- BP算法参数的更新过程（这一部分在附录里，算是补充内容，姑且算作第三部分吧）

其中，因为很多方式的计算过程是一样的，所以论文着重点在第一部分，准确说是 CBOW模型的在单个输入的情况下参数的更新过程。下面简单介绍我在学习论文时候的一些小问题，当然，具体的计算过程还是要看原论文。



#### 第一部分 CBOW模型以及Skip-Gram模型的参数计算过程
我觉得注意点有：

 - 首先要明确的是几个参数的含义，像 $V$ 表示的是我们词汇表的长度， $N$ 是最终训练得到的词向量的维数，也就是对于每个单词（字），对应的是一个 (1, N) 或者说是 (N, 1) 的词向量。

 - 第二点要注意这里计算过程中使用的向量都是列向量的形式，例如输入是一个 (V, 1) 的 one-hot 编码的列向量，在 input vector 和 output vector 之间的中间结果也是一个 (N, 1)的列向量，可能是我刚开始学习论文的原因，总觉得这样计算过程不算很直观，这样的结果是公式中出现了很多转置符号来保证结果的尺寸，不过，要理解计算过程也不难，例如 在得到中间变量 $h$ 的计算中： $h = WT.x$ 这里结合每一部分的意义来理解，也就是从矩阵 $W$ （也就是 input vector 矩阵）中拿出来要训练的那个词向量。第二个过程也是类的意义，其实理解一下矩阵乘法就好。

 - 第三点是参数的更新过程，注意这里的损失函数的定义方式，在这里，对于输入的一个样本，输出的是一个长度为 $V$ 的向量，表示的是对于这个输入的样本属于某个单词的度量，例如 $V = (1.2,\  3.4,\  0.3,\  -2.3...)$，当然为了更直观，这些数据会通过softmax方式处理为一个概率分布，因此，将目标定为使得正确的分类概率最大即可，也就是论文中的：
$$
max(w_{O}|w_{I})= max \ y_{j^{*}}
$$
这里的 $y_{j^{*}}$ 也就是输入数据正确对应的分类（具体来说，比如在“天气好”，如果分成“天气”和“好”两个词语的话，在CBOW中可能是使用“天气”来预测“好”，那么输入是“天气”，那么经过前向计算结果的正确分类就应该是“好”这个词语，对于词汇表来说，其中的每一个词都是一个类别，只要让“好”这个类别的概率最大就可以了）。使用最大似然中的方式，上式的最大值也就是求取对数之后式子的最大值，而且，按照一般定义损失函数最小的惯例，将上面的式子取负数,就得到了使用的损失函数：
$$
E = log\sum_{j^{j}}^{V}exp(u_{j^{'}}) - u_{j^{*}}
$$
接下来文章介绍的是参数更新的过程，主要也就是两个矩阵的更新过程，根据链式法则对上面的损失函数求梯度，这里注意上面的损失函数中的$u_{j^{*}}$是指的正确的分类，所以，在计算某个$u_{j}$的梯度的时候，这一项只有在正确的类别的时候，求导的结果才会使1，其余的情况都是0.这一点在论文中也做了解释。
文中为了简洁起见，对一些计算的过程重新命了名，这一点在推导时候稍微注意。

 - 在这一节的后半部分，介绍了多输入情况下的计算，其实计算过程是一样的，只是对多个输入向量取了个均值，后面的计算过程就与单个输入的计算一样了。实际上，在实际计算词向量的时候，对多输入的情况就是取均值，很简单但是也很有效的方式。

 - 对于 Skip-Gram Model，从介绍的页数上就可以看出来，计算过程与之前的也是差不多的，注意这里的损失函数的定义，因为Skip-Gram是使用一个中心词来预测上下文词语，在论文的公式中，使用的一个词来预测上下文中的 $C$个词语，对于公式(27)-(29），我们定义的目标函数是联合概率：$p(w_{O,1},w_{O,2},...,w_{O,C}|w_{I})$最大，假设这里每一个预测的过程是独立的，那么上面的式子展开也就是公式（28），可以看到，么一部分的计算形式与之前的CBOW中是一样的。那么最终的优化目标就是权衡这$C$个词语，使它们总体的预测性能最好。
#### 第二部分 两种优化计算方式
在实际使用中，是不会像上面一样计算词向量的因为计算的规模太大了，所以也就有了两种优化的方式，第一种是Hierarchical
 Softmax方式，也就是 层次 Softmax，这种方式是利用Huffman树进行减少计算的，我们知道Huffman编码可以根据词频产生编码使得全部编码的长度最小，这里的叶子节点也就是词汇表中的词语，每一个词语对应一个叶子节点，这一部分在论文中介绍的很清楚，具体的请参考论文，我觉得要注意的地方在有以下几个：

 - 首先是从一个节点出发，走到左节点或者右节点的概率和是为1的（好像很多余。。。），在这里对于每一个节点会使用一个函数$\delta$来将数据处理为一个$(0-1)$区间内的值，这个函数称为：Logistic 函数（其实就是Sigmoid函数 $\delta(u)=\frac{1}{1+e^{-u}}$）. 在论文中规定左分支是1，右分支为0，那么在一个节点选择1的概率是$\delta(x,h)$的话，那么选择右分支的概率就是$1-\delta(x\cdot h)$，然后根据Logistic 函数的特点：$1-\delta(u)=\delta(-u)$ ，所以右分支就可以表示为$\delta(-x\cdot h)$。这一部分在论文中还介绍了一个详细的例子。
 - 第二点是关于公式中的$h$，这里的$h$也就是输入的向量，单个输入就是本身，如果是在CBOW中多个输入的情况，也就是取均值之后的结果。这个$h$注意，在所有的节点计算中都是一样的，也就是原始输入，并非是下面层的节点计算使用上面的结果。
 - 最终每一个节点的概率也就是路径上每一部分连乘的结果，其实这样的过程就是分割每一个节点概率的过程（好像不太好表述），就是公式（43）的计算。
 - 

第二中优化方法是 Negative Sampling 方式，也就是负采样，其实使用层次 Softmax 方式，计算量依然很大（查资料也可以发现，使用负采样的案例更多=_=||），这种方式更好理解，既然要减少计算量，原本我们在全部的词汇表(共有$V$个词)中，只有一个是正确的，其他的都是负例，现在我们计算负例的时候只使用一部分，这也就是对负例进行采样，所以就是“负采样”了（个人解释），在这一部分中，损失函数的思路与之前一样，也就是让$p_{+}\cdot p_{-}\cdot p_{-}\cdot \cdot \cdot$这个式子的值最大，这一部分在论文中没有直接提到，其实就是公式（55）的原始形式吧，最开始也就是：
$$
p_{+}\cdot p_{-}\cdot p_{-}\cdot \cdot \cdot = \delta(v_{w_{O}}^{'} h)\prod_{w_{j}\in neg}\delta(-v_{w_{j}}^{'} h)
$$
当然，这里的$\delta(-v_{w_{j}}^{'} h)$是根据函数($1-\delta(v_{w_{j}}^{'} h)$)的特点变换得到的。然后将上面的式子取对数，然后添负号把求最大值变成求最小值，结果也就是论文中的公式(55)了。

#### 第三部分 附录：反向传播算法推导
这一部分并不是论文的重点，这里对反向传播算法的参数更新过程做了推导，如果对前面word2vec中两个参数矩阵的参数更新方法不清楚，可以先看一下这一部分，写的很直观。

